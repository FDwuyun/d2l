{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" \n",
    "实战Kaggle比赛：classify leaf \n",
    "https://www.kaggle.com/competitions/classify-leaves\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、下载数据集 \n",
    "# kaggle competitions download -c classify-leaves\n",
    "\n",
    "# 二、读取数据集\n",
    "# current_file_dir = os.getcwd()\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# print(current_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18353, 2)\n",
      "(8800, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "训练数据集包括18353个样本，每个样本个特征和1个标签，\n",
    "而测试数据集包含8800个样本，\n",
    "共176种类\n",
    "\"\"\"\n",
    "train = pd.read_csv(os.path.join(script_dir, \"data\", \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(script_dir, \"data\", \"test.csv\"))\n",
    "# print(train.shape)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = list(train[\"label\"])\n",
    "# 获取训练集中标签的种类\n",
    "labels_categories = list(set(train_labels))\n",
    "# 将训练集对应的标签转化为数字索引 labels_categories -> index\n",
    "labels_num = []\n",
    "for i in range(len(train_labels)):\n",
    "    labels_num.append(labels_categories.index(train_labels[i]))\n",
    "\n",
    "train[\"number\"] = labels_num\n",
    "# print(train.shape)\n",
    "# index=False 不在文件的第一列加索引\n",
    "# train.to_csv(\"./data/train_num_label.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三、预处理数据\n",
    "class Leaf_Train_Dataset(Dataset):\n",
    "    '''\n",
    "    树叶数据集的训练集 自定义Dataset\n",
    "    '''\n",
    "    def __init__(self, train_path, transform = None) -> None:\n",
    "        '''\n",
    "        train_path : 传入记录图像路径及其标号的csv文件\n",
    "        transform : 对图像进行的变换\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.train_csv = pd.read_csv(train_path)\n",
    "        # 以列表的形式记录图像所在地址\n",
    "        self.images_list = list(self.train_csv[\"image\"])\n",
    "        # 图像的标号记录\n",
    "        self.label_nums = list(self.train_csv[\"number\"])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx : 所需要获取的图像的索引\n",
    "        return : image， label\n",
    "        '''\n",
    "        image = Image.open(os.path.join(\"/home/qlf/d2l/chapter7/7_8_classify_leaf/data\", self.images_list[idx]))\n",
    "        if(self.transform != None):\n",
    "            image = self.transform(image)\n",
    "        label = self.label_nums[idx]\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "    \n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.5 ,1), ratio=(3/4, 4/3)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # 这一句一定要放在下面这句的前面\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    #可以试试加上这个代码，%config InlineBackend.figure_format = 'svg'\n",
    "    # backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "#通过以上三个用于图形配置的函数，定义一个plot函数来简洁地绘制多条曲线， 因为我们需要在整个书中可视化许多曲线。\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"绘制数据点\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else plt.gca()\n",
    "\n",
    "    # 如果X有一个轴，输出True\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(7, 5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        plt.show()\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)  # 获得每行中最大元素的索引来获得预测类别\n",
    "    cmp = y_hat.type(y.dtype) == y  #\n",
    "    return float(cmp.type(y.dtype).sum())  # 返回预测正确的个数\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.lastTimeSum = 0\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "    \n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        self.lastTimeSum = sum(self.times)\n",
    "        return self.lastTimeSum\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 设置为评估模式，关闭Dropout和直接结算所有batch的均值和方差\n",
    "        if not device:\n",
    "            # 使用参数来构建一个虚拟的计算图，然后从计算图中获取一个参数张量，然后通过 .device 属性获取这个参数张量所在的设备。这个参数张量位于模型的第一个参数（通常是一个权重矩阵）。\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需要的\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "# 获取当前日期和时间\n",
    "current_datetime = datetime.datetime.now()\n",
    "# 将日期和时间格式化为字符串，例如：2023-09-10-14-30-00\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "log_dir = os.path.join(script_dir, \"logs\", f\"DenseNet_{formatted_datetime}\")\n",
    "\n",
    "# 实例化SummaryWriter对象\n",
    "tb_writer = SummaryWriter(log_dir = log_dir)\n",
    "# 实例化Logger对象\n",
    "sys.stdout = Logger(log_dir + f\"/output_{formatted_datetime}.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def train(net, train_loader, valid_loader, num_epochs, lr, device = try_gpu()):\n",
    "    if(net.parameters().__next__().device != device):\n",
    "        net.to(device)\n",
    "        print(\"training on \", device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    animator = Animator(xlabel=f'epoch, lr={lr}, , batch_size={batch_size}, {device}', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'valid acc'])    \n",
    "    timer  = Timer()\n",
    "\n",
    "    # num_batches 表示 total_samples / batch_size\n",
    "    num_batches = len(train_loader) \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        # 循环的次数为 num_batches\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                # 这部分代码计算了当前批次的损失乘以批次的大小（样本数量）。\n",
    "                # 这是为了得到当前批次的总损失。通常，损失是对单个样本的损失，将其乘以批次大小可以得到批次的总损失。\n",
    "                # X.shape[0] 经常= batch_size ，但最后一个loader一般小于batch_size\n",
    "                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                train_loss = metric[0] / metric[2]\n",
    "                train_acc = metric[1] / metric[2]\n",
    "                animator.add(\n",
    "                    epoch + (i + 1) / num_batches,\n",
    "                    (train_loss, train_acc, None)\n",
    "                )\n",
    "        valid_acc = evaluate_accuracy_gpu(net, valid_loader)\n",
    "        animator.add(epoch + 1, (None, None, valid_acc))  \n",
    "        print(f\"{timer.sum():.1f} sec,\", \"[epoch: {}] train_loss: {:.3f}, train_acc: {:.3f}, valid_acc: {:.3f}\".format(epoch, train_loss, train_acc, valid_acc))\n",
    "        \n",
    "        tb_writer.add_scalar(\"train_loss\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"train_acc\", train_acc, epoch + 1)\n",
    "        tb_writer.add_scalar(\"valid_acc\", valid_acc, epoch + 1)\n",
    "    \n",
    "    print(f\"[Total] loss {train_loss:.3f}, train acc {train_acc:.3f},\" f\"valid acc {valid_acc:.3f}\")\n",
    "    print(\n",
    "        f\"[Total] {timer.sum():.1f} sec, {metric[2] * num_epochs / timer.sum():.1f} examples/sec\"\n",
    "        f\"on {str(device)}\"\n",
    "    )\n",
    "    return train_loss, train_acc, valid_acc \n",
    "\n",
    "\"\"\"\n",
    "[**K折交叉验证**]有助于模型选择和超参数调整。\n",
    "在$K$折交叉验证过程中返回第$i$折的数据。\n",
    "具体地说，它选择第$i$个切片作为验证数据，其余部分作为训练数据。\n",
    "注意，这并不是处理数据的最有效方法，如果我们的数据集大得多，会有其他解决办法。\n",
    "\"\"\"\n",
    "\n",
    "def get_k_fold_dataset(k, i, X, y):\n",
    "    assert k > 1 , \"K折交叉验证 require k > 1\"\n",
    "    fold_size = X.shape[0] // k # //是向下取整\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        # slice() 函数实现切片对象，主要用在切片操作函数里的参数传递。\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            # torch.cat(tensors, dim=0, out=None) -> Tensor, 一个将张量沿着指定维度拼接起来的函数\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    \n",
    "    train_dataset = Leaf_Train_Dataset(X_train, y_train)\n",
    "    valid_dataset = Leaf_Train_Dataset(X_valid, y_valid)\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def k_fold_cross_train(k, X_train, y_train, num_epochs, lr, batch_size):\n",
    "    train_loss, train_acc, valid_acc = [], [], []\n",
    "    # train_loss_sum, train_acc_sum, valid_acc_sum = 0, 0, 0\n",
    "\n",
    "    for i in range(k):\n",
    "        train_dataset, valid_dataset = get_k_fold_dataset(k, i, X_train, y_train, batch_size)\n",
    "        train_loader = data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        valid_loader = data.DataLoader(valid_dataset, batch_size, shuffle=True)\n",
    "        print(f'折{i + 1}')\n",
    "        train_loss[i], train_acc[i], valid_acc[i] = train(net, train_loader, valid_loader,\n",
    "                     num_epochs, lr, try_gpu())\n",
    "        \n",
    "    return sum(train_loss) / k, sum(train_acc) / k, sum(valid_acc) / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择网络\n",
    "\n",
    "resnet50 = models.resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Leaf_Train_Dataset(os.path.join(script_dir, \"data\", \"train_num_label.csv\"), transform = transforms_train)\n",
    "# valid_dataset = Leaf_Train_Dataset(os.path.join(script_dir, \"data\", \"test.csv\"), transform = transforms_test)\n",
    "# train_loader = data.DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "# valid_loader = data.DataLoader(valid_dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "train(resnet50, train_loader, valid_loader, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测并生成csv文件\n",
    "train_dataset = Leaf_Train_Dataset(os.path.join(script_dir, \"data\", \"train_num_label.csv\"), transform = transforms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-06-20-35-27'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    # 获取当前日期和时间\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # 将日期和时间格式化为字符串，例如：2023-09-10-14-30-00\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    return formatted_datetime\n",
    "get_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
